#  ETL con Airflow

## 1) Objetivo

Construir un pipeline **ETL** orquestado con **Apache Airflow** que:

* Extrae Spotify (CSV) y Grammys (DB).
* Limpia, normaliza y **fusiona** ambos datasets.
* Exporta el resultado a **Google Drive** (CSV).
* Carga un **esquema estrella** en MySQL para anal√≠tica y dashboarding.

---

## 2) Estructura del proyecto

```
AIRFLOW/
‚îÇ
‚îú‚îÄ‚îÄ dags/                          # Carpeta principal donde se almacenan los DAGs de Airflow
‚îÇ   ‚îú‚îÄ‚îÄ __pycache__/               
‚îÇ   ‚îú‚îÄ‚îÄ data/                      # Carpeta interna para datos generados dentro del DAG
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ spotify_grammy_full.csv     # Archivo CSV final generado tras la transformaci√≥n y merge
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ etl_pipeline.py            # DAG principal con todas las tareas ETL
‚îÇ   ‚îú‚îÄ‚îÄ get_token.py               # Script auxiliar para obtener credenciales de Google Drive
‚îÇ   ‚îú‚îÄ‚îÄ credentials.json           # Credenciales OAuth de la API de Google Drive
‚îÇ   ‚îî‚îÄ‚îÄ token.json                 # Token de autenticaci√≥n generado tras autorizar Google Drive
‚îÇ
‚îú‚îÄ‚îÄ data/                          # Carpeta con los datasets originales
‚îÇ   ‚îú‚îÄ‚îÄ spotify_dataset.csv             # Dataset original de Spotify
‚îÇ   ‚îî‚îÄ‚îÄ the_grammy_awards.csv           # Dataset original de los premios Grammy
‚îÇ
‚îú‚îÄ‚îÄ logs/                          # Registros generados por las ejecuciones del DAG
‚îÇ
‚îú‚îÄ‚îÄ plugins/                       # Carpeta reservada para futuros plugins o hooks personalizados
‚îÇ
‚îú‚îÄ‚îÄ EDA.ipynb                      # Notebook de an√°lisis exploratorio (EDA)
‚îú‚îÄ‚îÄ load_raw_grammy.py             # Script para cargar los datos del Grammy a MySQL
‚îú‚îÄ‚îÄ docker-compose.yaml            # Configuraci√≥n del entorno Docker de Airflow
‚îú‚îÄ‚îÄ requirements.txt               # Dependencias del proyecto (rapidfuzz, pydrive2, etc.)
‚îî‚îÄ‚îÄ .env                           # Variables de entorno locales (conexiones, claves, etc.)

```

---

## 3) Dise√±o del DAG (Airflow)

**DAG:** `etl_pipeline` (daily, sin catchup)

**Tareas y dependencias:**

```mermaid
graph LR
  A[extract_spotify_csv] --> C[transform_and_merge]
  B[extract_grammy_db]  --> C[transform_and_merge]
  C --> D[load_to_drive]
  C --> E[load_star_schema]
```
---

## Dise√±o del DAG `etl_pipeline`

El DAG **`etl_pipeline`** orquesta todo el proceso de extracci√≥n, transformaci√≥n y carga de los datos de **Spotify** y **los premios Grammy** dentro de un entorno controlado por **Apache Airflow**. Su prop√≥sito es automatizar la integraci√≥n de ambas fuentes, aplicar transformaciones de limpieza y emparejamiento difuso, y finalmente cargar la informaci√≥n resultante en una base de datos con un modelo estrella para an√°lisis posterior.

Este flujo se ejecuta de manera diaria (`@daily`), sin ejecuciones acumuladas (`catchup=False`) y con un m√°ximo de una instancia activa a la vez (`max_active_runs=1`), lo que garantiza estabilidad durante la carga. El pipeline se apoya en dos conexiones definidas dentro de Airflow: `mysql_local`, que accede a la tabla de origen `grammy_awards`, y `mysql_dw`, que conecta con el data warehouse destino.

En l√≠neas generales, el DAG sigue esta secuencia:

1. **Extrae los datos originales** de Spotify (desde un archivo CSV local) y de los Grammy (desde una base de datos MySQL).
2. **Transforma y fusiona** ambas fuentes aplicando procesos de limpieza, normalizaci√≥n y coincidencia difusa para identificar canciones nominadas.
3. **Exporta el dataset resultante** a un archivo CSV y lo sube autom√°ticamente a Google Drive.
4. **Carga los datos finales** en un modelo estrella en MySQL para facilitar el an√°lisis y la visualizaci√≥n.

Las tareas que componen el DAG son las siguientes:

### `extract_spotify_csv`

Esta tarea lee el archivo `spotify_dataset.csv` ubicado en `/opt/airflow/data/` y lo convierte en un formato JSON que se transfiere entre tareas.

### `extract_grammy_db`

Aqu√≠ se establece una conexi√≥n local con la base de datos `mysql_local` utilizando el hook de Airflow. Se ejecuta una consulta SQL sobre la tabla `grammy_awards` para recuperar toda la informaci√≥n de las nominaciones. Los resultados se transforman tambi√©n a formato JSON.

### `transform_and_merge`

Enesta etapa se limpian, normalizan y alinean ambas fuentes de datos.
Primero, el dataset de Spotify pasa por un proceso de depuraci√≥n: se eliminan columnas irrelevantes, filas nulas y duplicadas; se ajustan los nombres de g√©neros musicales para agruparlos en categor√≠as principales; y se corrigen valores an√≥malos como duraciones o niveles de volumen.
Luego, los datos de los Grammy se normalizan y se preparan para el emparejamiento. El DAG utiliza la librer√≠a **RapidFuzz** para comparar t√≠tulos y artistas entre ambos conjuntos y detectar coincidencias aun cuando los nombres presenten peque√±as variaciones. El resultado es un nuevo dataset unificado con una columna adicional (`grammy_nominee`) que marca las canciones nominadas. Finalmente, el archivo consolidado se guarda en la ruta `/opt/airflow/dags/data/spotify_grammy_full.csv`, convirti√©ndose en el producto principal del proceso.

### `load_to_drive`

Una vez generado el archivo final, esta tarea se encarga de subirlo autom√°ticamente a **Google Drive** mediante la API oficial de Google. Usa las credenciales almacenadas en `credentials.json` y `token.json` para autenticar la sesi√≥n, refrescando el token si es necesario. El archivo se guarda en la carpeta configurada mediante su id de drive.

### `load_star_schema`

La √∫ltima etapa construye y llena el **modelo estrella** dentro de la base de datos conectada como `mysql_dw`. Se crean tablas de dimensiones para pistas, artistas, √°lbumes, g√©neros, a√±os y nominaciones, adem√°s de una tabla de hechos con m√©tricas musicales y la bandera de nominaci√≥n.

---

## 4) Pipeline ETL ‚Äî Explicaci√≥n de pasos

### 4.1 Extracci√≥n

* **Spotify (CSV)**

  * Se lee archivo local y se retorna como JSON para pasar datos entre tareas.
* **Grammys (DB)**

  * Se ejecuta `SELECT * FROM grammy_awards;` con `MySqlHook(mysql_local)` y se retorna JSON.

### 4.2 Transformaci√≥n y Fusi√≥n (`transform_and_merge`)

**Sobre Spotify:**

* Elimina columna auxiliar `Unnamed: 0` si existe.
* Remueve columnas no requeridas: `key`, `mode`, `time_signature`, `winner`.
* Borra filas con nulos y duplicados (estricto para asegurar consistencia).
* Normaliza g√©neros: mapea **sub-g√©neros** a **main buckets** (e.g., *alt-rock ‚Üí rock*, *deep-house ‚Üí electronic*, *k-pop ‚Üí pop*, etc.).
* Crea `sub_genre` (original) y `main_genre` (mapeada). Quita `track_genre`.
* **Resoluci√≥n de duplicados por `track_id`:** conserva el registro con mayor `popularity` y concatena sub-g√©neros secundarios en `sub_genre`.
* Convierte `duration_ms` a **minutos** ‚Üí `duration_min`.
* Limita `loudness` a `‚â§ 0` (corrige valores positivos an√≥malos).
* Estandariza texto en `artists`, `album_name`, `track_name` (min√∫sculas, trim).
* Consolida por `(track_name, artists)` conservando el m√°s popular y listando en una nueva columna `album_others` para no perder dimensionalidad.

**Sobre Grammys (CSV/DB):**

* Elimina columnas no √∫tiles para el enfoque: `winner`, `workers`, `img`, `published_at`, `updated_at`.
* Normaliza texto en `title`, `category`, `nominee`, `artist`.
* Llena vac√≠os en `nominee`/`artist` con `not specified`.

**Emparejamiento Spotify ‚Üî Grammys:**

* Antes de combinar ambas fuentes, se preparan los campos clave para asegurar comparaciones precisas.
  En el caso de los artistas, se construyen listas de nombres ‚Äúlimpios‚Äù eliminando tildes y caracteres especiales, pasando todo a min√∫sculas y reemplazando conectores como ‚Äúfeat.‚Äù, ‚Äúfeaturing‚Äù, ‚Äú&‚Äù o comas por un √∫nico separador `;`.
  Esto permite que colaboraciones escritas de forma diferente (‚ÄúBeyonc√© feat. Jay-Z‚Äù vs. ‚ÄúJay Z & Beyonc√©‚Äù) sean tratadas como equivalentes.

* Se crea una bandera llamada `is_various_artists` que identifica los registros de los Grammy correspondientes a √°lbumes o compilaciones con m√∫ltiples artistas.

* Una vez normalizados los datos, se realiza un **merge inicial** entre Spotify y Grammy utilizando un *outer join* entre los campos `track_name` (Spotify) y `nominee` (Grammy).
  Este tipo de uni√≥n conserva todos los posibles pares de coincidencia, incluso si existen diferencias menores en los nombres.

* Sobre los resultados de ese merge se aplica un proceso de **fuzzy matching** utilizando la librer√≠a **RapidFuzz**, con el objetivo de identificar coincidencias aproximadas entre texto.
  Se compara primero el t√≠tulo de la canci√≥n mediante el m√©todo `token_set_ratio`, que ignora el orden de las palabras y peque√±as variaciones tipogr√°ficas.
  Se define un umbral de similitud del **90%** para considerar que los t√≠tulos representan la misma obra.
  Luego, se procede con la comparaci√≥n de artistas:

  * Si el registro de Grammy corresponde a `is_various_artists=True`, se valida √∫nicamente que el lado de Spotify tenga **m√°s de un artista** (ya que representa un conjunto).
  * En los dem√°s casos, se comparan las listas de artistas de ambos lados y se considera coincidencia si **al menos una pareja** supera el mismo umbral del 90%.

* Cuando una fila cumple ambas condiciones ‚Äîcoincidencia de t√≠tulo y coincidencia de artista‚Äî se genera la bandera **`grammy_nominee=True`**, indicando que esa canci√≥n o √°lbum de Spotify fue identificado como nominado en los Grammy.

* Despu√©s del emparejamiento, se ajustan los tipos de datos y se completan los valores faltantes para mantener la integridad del dataset:

  * `explicit` se convierte a booleano y se rellena con `False` por defecto.
  * Las columnas de texto vac√≠as se reemplazan con `"not specified"`.
  * Los valores num√©ricos nulos se rellenan con `0` para evitar errores en la carga al modelo estrella.

* Finalmente, se renombran las columnas para distinguir claramente su origen ‚Äîpor ejemplo, `artist_spotify` y `artist_grammy`‚Äî y se organizan en un orden l√≥gico (variables de Spotify primero, luego las de Grammy y por √∫ltimo las auxiliares).
  El resultado es un **CSV consolidado** almacenado en `OUT_PATH`, que sirve como producto final de la etapa de transformaci√≥n y la base para las siguientes fases del pipeline.

---
## 4.3 Carga (outputs)

**`load_to_drive`**
Publica el artefacto final del ETL (el CSV consolidado) en **Google Drive**. Primero valida que la ruta exista; luego lee el token desde `TOKEN_PATH`, refresc√°ndolo si expir√≥. Con las credenciales activas, construye el cliente de la **Drive API v3**, define metadatos y, si se configur√≥ `FOLDER_ID`, sube el archivo **dentro de esa carpeta**. La subida usa `MediaFileUpload` en modo *resumable* para tolerar microcortes.

**`load_star_schema`**
Carga el dataset final en un **modelo estrella MySQL**. Lee el CSV consolidado y normaliza campos booleanos que pudieran venir como texto. Abre la conexi√≥n `mysql_dw` y ejecuta el **DDL** del DW. Si `recreate_schema=True`, recrea el esquema de cero (ideal en desarrollo). Crea/asegura las **dimensiones** (`dim_track`, `dim_artist`, `dim_album`, `dim_genre`, `dim_time`, `dim_grammy`). Inserta las dimensiones con `to_sql`, recupera las *surrogate keys* y construye la **tabla de hechos `fact_track_metrics`** uniendo por claves naturales. Convierte `explicit` y `grammy_nominee` a `INT` (0/1) y hace la carga por lotes (`chunksize=10_000`) para buen rendimiento.

---

## 5) Esquema Estrella (MySQL)

### Tablas de Dimensiones

Las dimensiones contienen informaci√≥n descriptiva o categ√≥rica, y cada una tiene su propia clave primaria (`*_key`) que se usa para relacionarse con la tabla de hechos:

* **`dim_track`**: almacena los datos b√°sicos de cada canci√≥n: su ID de Spotify y su nombre.
* **`dim_artist`**: lista todos los artistas √∫nicos presentes en el dataset.
* **`dim_album`**: guarda los nombres de los √°lbumes y, si una canci√≥n pertenece a varios, los agrupa en `album_others`.
* **`dim_genre`**: contiene los g√©neros principales (`main_genre`) y subg√©neros (`sub_genre`), sin duplicados.
* **`dim_time`**: almacena los a√±os de publicaci√≥n o nominaci√≥n, para analizar tendencias a lo largo del tiempo.
* **`dim_grammy`**: guarda informaci√≥n de las nominaciones: t√≠tulo, categor√≠a, nominado y artista asociado.
  Adem√°s, tiene una **clave √∫nica generada con un hash (SHA2)** que evita que se repitan registros cuando los textos tienen peque√±as diferencias.

---

### Tabla de Hechos

La tabla **`fact_track_metrics`** almacena los valores num√©ricos y las m√©tricas de an√°lisis.
Cada registro representa una canci√≥n y sus atributos musicales, junto con una marca que indica si fue nominada a los Grammy.

Contiene referencias (`Foreign Keys`) a todas las dimensiones. Sus campos principales son:

* **Claves for√°neas:** `track_key`, `artist_key`, `album_key`, `genre_key`, `time_key`, `grammy_key`.
* **M√©tricas num√©ricas:** `popularity`, `duration_min`, `danceability`, `energy`, `loudness`, `valence`, `tempo`, entre otras.
* **Banderas booleanas:**

  * `explicit` (0 o 1 seg√∫n si la canci√≥n tiene contenido expl√≠cito).
  * `grammy_nominee` (0 o 1 seg√∫n si fue nominada a un Grammy).

---
## 6) Suposiciones y Decisiones Clave

* **G√©nero principal vs. subg√©nero:**
  Se cre√≥ una jerarqu√≠a de g√©neros para mejorar la consistencia anal√≠tica.
  El campo `main_genre` agrupa subg√©neros similares (por ejemplo, *alt-rock*, *punk-rock* y *grunge* como *rock*), lo que permite comparar categor√≠as generales en los dashboards.
  El `sub_genre` conserva el detalle original para an√°lisis m√°s espec√≠ficos. Esta decisi√≥n equilibra **precisi√≥n y legibilidad**.

* **Duplicados por `track_id` y (`track_name`, `artists`):**
  Muchas canciones aparec√≠an repetidas con el mismo artista pero en diferentes √°lbumes (como versiones remasterizadas o recopilatorios).
  Se decidi√≥ conservar **solo la versi√≥n m√°s popular**, ya que representa la m√°s relevante para el usuario, y agrupar por `track_name` y `artists` para evitar m√∫ltiples coincidencias en el merge con los Grammy.
  Los √°lbumes restantes se concatenaron en `album_others`. Esto garantiza que cada canci√≥n-artista tenga **una sola representaci√≥n** en el dataset final.

* **Duraci√≥n y Loudness:**
  La duraci√≥n se convirti√≥ de milisegundos a minutos (`duration_min`) para hacerla m√°s comprensible.
  Adem√°s, los valores positivos de `loudness` se ajustaron a 0, ya que los decibelios relativos no suelen ser positivos.
  Ambas decisiones mejoran la **interpretabilidad y coherencia de las m√©tricas**.

* **Normalizaci√≥n de texto y artistas:**
  Se pasaron todos los textos a min√∫sculas, sin tildes ni espacios extra, y se unificaron conectores como ‚Äúfeat.‚Äù o ‚Äú&‚Äù bajo un mismo formato `;`.
  Esto asegura que las listas de artistas sean **comparables y consistentes**, evitando errores durante el emparejamiento.

* **Emparejamiento difuso (RapidFuzz):**
  Para vincular correctamente las canciones de Spotify con las nominaciones Grammy, se aplic√≥ un **fuzzy matching** con umbral del 90%.
  Se compararon t√≠tulos (`token_set_ratio ‚â• 90`) y listas de artistas bajo las mismas condiciones.
  En los casos de ‚Äúvarious artists‚Äù, se exigi√≥ que la canci√≥n tuviera varios int√©rpretes en Spotify.
  As√≠ se equilibr√≥ **precisi√≥n y flexibilidad**, reduciendo falsos positivos.

* **Nulos y tipos de datos:**
  Los valores faltantes se rellenaron con `"not specified"`, `0` o `False` seg√∫n el tipo de dato.
  
---

## 7) Visualizaciones y KPIs (Resumen)

*Secci√≥n para el dashboard conectado al DW (Power BI / Looker / Tableau):*

* **KPIs sugeridos:**

  * # de **matches** Spotify ‚Üî Grammys (`grammy_nominee=True`).
  * **Top artistas** por popularidad y/o por nominaciones.
  * **Tendencia anual** de nominaciones vs. popularidad promedio.
* **Gr√°ficas sugeridas:**

  * Barras: nominaciones por **main_genre**.
  * L√≠nea: popularidad media por **a√±o**.
  * Mapa de calor: **correlaciones** entre m√©tricas de audio y bandera `grammy_nominee`.
  * Dispersi√≥n: **popularidad vs. tempo/loudness**, coloreado por `grammy_nominee`.

---

## 8) Setup e Instrucciones de Ejecuci√≥n

Esta secci√≥n explica c√≥mo instalar, configurar y ejecutar el pipeline ETL con **Apache Airflow**, **Docker** y **MySQL**, adem√°s de c√≥mo conectar la autenticaci√≥n de **Google Drive** para la carga autom√°tica del CSV final.

---

### 1. Requisitos previos

Antes de iniciar, aseg√∫rate de tener instalado:

* **Docker Desktop** (con Docker Compose habilitado)
* **Python 3.9+** (solo si planeas generar el token de Google Drive fuera del contenedor)
* **Git** (para clonar el repositorio)
* **MySQL** instalado localmente o como servicio externo (para el Data Warehouse)

---

### 2. Clonar el repositorio

```bash
git clone https://github.com/<tu_usuario>/<tu_repositorio>.git
cd AIRFLOW
```

Aseg√∫rate de mantener esta estructura de carpetas (ya mapeada en los vol√∫menes del contenedor):

```
AIRFLOW/
‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îú‚îÄ‚îÄ etl_pipeline.py
‚îÇ   ‚îú‚îÄ‚îÄ get_token.py
‚îÇ   ‚îú‚îÄ‚îÄ credentials.json
‚îÇ   ‚îú‚îÄ‚îÄ token.json
‚îÇ   ‚îî‚îÄ‚îÄ data/
‚îÇ       ‚îî‚îÄ‚îÄ spotify_grammy_full.csv  ‚Üê se genera al ejecutar el DAG
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ spotify_dataset.csv
‚îÇ   ‚îî‚îÄ‚îÄ the_grammy_awards.csv
‚îú‚îÄ‚îÄ docker-compose.yaml
‚îî‚îÄ‚îÄ requirements.txt
```
---

### 3. Configurar credenciales de Google Drive

Para permitir que Airflow suba el archivo final a Google Drive, debes crear un proyecto en Google Cloud Console, habilitar la API de Drive y generar credenciales OAuth.

#### Paso 1: Crear un proyecto en Google Cloud Console

1. Ingresa a [https://console.cloud.google.com](https://console.cloud.google.com) e inicia sesi√≥n con tu cuenta de Google.
2. En la esquina superior izquierda, haz clic en **‚ÄúSeleccionar proyecto‚Äù ‚Üí ‚ÄúNuevo proyecto‚Äù**.
3. Asigna un nombre, por ejemplo: `airflow-drive-uploader`.
4. Espera unos segundos hasta que el proyecto se cree y se seleccione autom√°ticamente.

#### Paso 2: Habilitar la API de Google Drive

1. Dentro del proyecto, ve al men√∫ lateral y selecciona:
   **‚ÄúAPIs y servicios ‚Üí Biblioteca‚Äù**.
2. Busca **Google Drive API**.
3. Haz clic sobre ella y presiona **‚ÄúHabilitar‚Äù**.

   > Esto permitir√° que Airflow pueda interactuar con tu Google Drive.

#### Paso 3: Crear credenciales OAuth

1. En el men√∫ lateral, entra a **‚ÄúAPIs y servicios ‚Üí Credenciales‚Äù**.
2. Haz clic en **‚Äú+ Crear credenciales‚Äù ‚Üí ‚ÄúID de cliente de OAuth‚Äù**.
3. Si te pide configurar una pantalla de consentimiento, selecciona **‚ÄúExterno‚Äù** y completa los campos m√≠nimos (solo el nombre de la app es obligatorio).
4. Luego, crea el ID de cliente de OAuth con:

   * **Tipo de aplicaci√≥n:** Aplicaci√≥n de escritorio
   * **Nombre:** Airflow Drive ETL (o el nombre que prefieras)
5. Una vez creado, descarga el archivo JSON de tus credenciales.

#### Paso 4: Guardar `credentials.json` y generar el token

1. Renombra el archivo descargado a **`credentials.json`** y gu√°rdalo en la carpeta:

   ```
   AIRFLOW/dags/credentials.json
   ```

2. En tu m√°quina local, ejecuta:

   ```bash
   cd dags
   python get_token.py
   ```

3. Se abrir√° una ventana del navegador solicitando acceso a tu cuenta de Google.
   Tras autorizar, se crear√° autom√°ticamente el archivo **`token.json`** en la misma carpeta.

4. Finalmente, en tu DAG (`etl_pipeline.py`), aseg√∫rate de definir correctamente las rutas:

   ```python
   TOKEN_PATH = "/opt/airflow/dags/token.json"
   FOLDER_ID  = "TU_FOLDER_ID_AQUI"   # Reemplaza con el ID real de tu carpeta en Drive
   ```

---

### 4. Construir e iniciar Airflow con Docker

Tu archivo `docker-compose.yaml` ya incluye todos los servicios necesarios (webserver, scheduler, worker, redis, postgres, etc.).

Ejecuta los siguientes comandos desde la ra√≠z del proyecto (`AIRFLOW/`):

```bash
docker compose up airflow-init
docker compose up -d
```

Esto inicializa Airflow, crea el usuario administrador y levanta el entorno en **[http://localhost:8080](http://localhost:8080)**.

> *Tip:* Puedes verificar que los contenedores est√©n activos con:
>
> ```bash
> docker ps
> ```
---

### 5. Crear base de datos `grammy_awards` y cargar CSV **raw**

 **Crear la base de datos en MySQL Workbench (manual):**

   * Abre **MySQL Workbench** y con√©ctate a tu servidor local.
   * Ejecuta:

     ```sql
     CREATE DATABASE IF NOT EXISTS grammy_awards
       DEFAULT CHARACTER SET utf8mb4
       COLLATE utf8mb4_unicode_ci;
     ```
   * Verifica que el esquema `grammy_awards` qued√≥ creado.

   * Ejecuta el script desde la ra√≠z del proyecto:

     ```bash
     python load_raw_grammy.py
     ```

---

### 6. Configurar conexiones en Airflow

1. Inicia Airflow (se explicar√° en el paso siguiente).
2. Ingresa al panel en tu navegador:
   [http://localhost:8080](http://localhost:8080)
   Usuario: `airflow`
   Contrase√±a: `airflow`
3. Ve a **Admin ‚Üí Connections** y crea las siguientes conexiones:

   * **MySQL Local (grammy source)**

     * Conn ID: `mysql_local`
     * Conn Type: MySQL
     * Host: `host.docker.internal`
     * Port: `3306`
     * Database: grammy_awards
     * Login / Password: credenciales locales

   * **MySQL Data Warehouse (destino)**

     * Conn ID: `mysql_dw`
     * Conn Type: MySQL
     * Host: `host.docker.internal` 
     * Port: `3306`
     * Database: `full_dw`
     * Login / Password: credenciales de tu DW

Estas conexiones permiten a Airflow leer los datos del Grammy y cargar el modelo estrella final en el DW.

### üîπ 6. Ejecutar el DAG ETL

1. Ingresa al panel de Airflow (`http://localhost:8080`).
2. Activa el DAG llamado **`etl_pipeline`**.
3. Haz clic en **Trigger DAG** para ejecutarlo manualmente.
4. El pipeline se ejecutar√°.
   
Al finalizar:

* El CSV final aparecer√° en `/opt/airflow/dags/data/spotify_grammy_full.csv`
* Tambi√©n se subir√° autom√°ticamente a Google Drive (carpeta definida por el `FOLDER_ID` que le especifiques)
* El modelo estrella se cargar√° en tu base de datos `full_dw`

---

###  7. Verificar resultados

* En Google Drive: busca el archivo **`spotify_grammy_full.csv`**.
* En MySQL: verifica las tablas con:

  ```sql
  USE full_dw;
  SHOW TABLES;
  SELECT COUNT(*) FROM fact_track_metrics;
  ```
* En Airflow: revisa la pesta√±a **Graph View** para visualizar el flujo de tareas y su estado.

---

### üîπ 8. Detener o reiniciar el entorno

Para detener todos los servicios:

```bash
docker compose down
```

Para limpiar completamente (incluyendo vol√∫menes y base de datos):

```bash
docker compose down --volumes --remove-orphans
```
